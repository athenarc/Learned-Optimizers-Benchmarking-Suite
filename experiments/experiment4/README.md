### **Experiment 4: Insights into Internal Decision-Making**

This experiment opens up the "black box" of LQOs to conduct a deep analysis of their core optimization logic. The goal is to understand *how* they make decisions regarding physical operators and *how well* their internal plan representations capture plan quality.

#### **4.1 Access Path Selection**

This sub-experiment assesses each LQO's ability to choose the optimal access path (Index Scan vs. Sequential Scan) under varying predicate selectivities.

**Methodology:**
1.  **Workloads:** Experiments are conducted on both the real-world **JOB** workload (with a new index added) and the simpler **JOB-Synthetic** workload.
2.  **Procedure:**
    *   For queries containing a filter on an indexed column, variants are generated with different predicate selectivities (from 10% to 100%).
    *   For each selectivity level, the optimal path is empirically determined by forcing both an Index Scan and a Sequential Scan and measuring their respective latencies.
    *   Each LQO then generates a plan for each query variant, and its chosen scan operator and resulting latency are recorded.
3.  **Metrics:**
    *   The LQO's scan choice is compared against the PostgreSQL baseline and the empirically optimal choice to evaluate its decision-making accuracy and identify any learned aggressive or conservative strategies.

#### **4.2 Physical Operator Composition**

This sub-experiment evaluates how each optimizer's strategy for selecting physical join operators (Hash, Nested Loop, Merge) evolves as query complexity increases.

**Methodology:**
1.  **Workload:** The JOB workload is partitioned into groups based on query complexity (e.g., queries with 3 joins, 5 joins, 7 joins, etc.).
2.  **Procedure:**
    *   For each complexity group, all queries are executed using each optimizer (including the classic baseline).
    *   The distribution of physical join operators selected by each optimizer is extracted and visualized.
3.  **Metrics:**
    *   **Operator Distribution:** The percentage of Hash, Nested Loop, and Merge joins chosen by each optimizer at different complexity levels.
    *   **Relative Speedup:** The end-to-end performance gain (or loss) for each group is calculated to correlate specific operator strategies with overall performance.

#### **4.3 Operator Bias Analysis**

This sub-experiment measures the accuracy of an LQO's internal predictions for specific, manually enforced join operators to identify systematic error patterns or "biases."

**Methodology:**
1.  **Procedure:**
    *   The JOB workload is partitioned by query complexity.
    *   For each query, three separate executions are performed with a different join operator (Nested Loop, Hash Join, Merge Join) manually enforced. This establishes the ground-truth latency for each operator choice.
    *   The prediction-based LQOs are then asked to predict the latency for each of these three operator-specific plans.
2.  **Metrics:**
    *   By plotting the **predicted latency vs. actual latency** for each operator type, we can identify systematic error patterns, such as a consistent tendency to over-predict the cost of Nested Loops or under-predict the cost of Hash Joins. These patterns explain *why* an optimizer might develop a preference for or against certain operators.

#### **4.4 Embedding Similarity and Plan Quality**

This final sub-experiment investigates the quality of the internal plan representations (embeddings) learned by the LQOs.

**Methodology:**
1.  **Procedure:**
    *   For a given query, two plans are considered: one generated by the LQO and one by PostgreSQL.
    *   The internal embeddings for both plans are extracted from the LQO's Value Model.
2.  **Metrics:**
    *   **Cosine Distance vs. Performance Difference:** The geometric distance (cosine distance) between the two plan embeddings is plotted against their actual performance difference. A high-quality embedding space should show a strong correlation, where geometrically distant plans also have large performance differences.
    *   **t-SNE Visualization:** The embeddings of all plans are projected into a 2D space using t-SNE to visually inspect whether the embedding space is well-structured (e.g., good plans cluster separately from bad plans), regardless of which optimizer generated them.